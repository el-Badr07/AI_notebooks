{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b95ae060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'sarcasm-json-datasets' dataset.\n",
      "Path to dataset files: /kaggle/input/sarcasm-json-datasets\n",
      "Path to dataset files: /kaggle/input/sarcasm-json-datasets\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"shariphthapa/sarcasm-json-datasets\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8af4a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /kaggle/input/sarcasm-json-datasets/Sarcasm.json -> shape (26709, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-6eba84c9-d497-4b6b-9e95-d9cdeae3bfdf\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6eba84c9-d497-4b6b-9e95-d9cdeae3bfdf')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6eba84c9-d497-4b6b-9e95-d9cdeae3bfdf button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6eba84c9-d497-4b6b-9e95-d9cdeae3bfdf');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "candidates = [\n",
    "    \"Sarcasm.json\",\n",
    "    os.path.join(path, \"Sarcasm.json\") if \"path\" in globals() else None,\n",
    "    os.path.join(path, \"Sarcasm_Headlines_Dataset.json\") if \"path\" in globals() else None,\n",
    "]\n",
    "file_path = next((p for p in candidates if p and os.path.exists(p)), \"Sarcasm.json\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "except ValueError:\n",
    "    df = pd.read_json(file_path)\n",
    "\n",
    "print(f\"Loaded {file_path} -> shape {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f3a050e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after dropping article_link: ['headline', 'is_sarcastic']\n",
      "\n",
      "Dataset shape: (26709, 2)\n",
      "\n",
      "First few rows:\n",
      "                                            headline  is_sarcastic\n",
      "0  former versace store clerk sues over secret 'b...             0\n",
      "1  the 'roseanne' revival catches up to our thorn...             0\n",
      "2  mom starting to fear son's web series closest ...             1\n",
      "3  boehner just wants wife to listen, not come up...             1\n",
      "4  j.k. rowling wishes snape happy birthday in th...             0\n",
      "\n",
      "Target distribution (is_sarcastic):\n",
      "is_sarcastic\n",
      "0    14985\n",
      "1    11724\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop article_link column and explore the dataset\n",
    "df_clean = df.drop(columns=['article_link'])\n",
    "print(\"Columns after dropping article_link:\", df_clean.columns.tolist())\n",
    "print(\"\\nDataset shape:\", df_clean.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_clean.head())\n",
    "print(\"\\nTarget distribution (is_sarcastic):\")\n",
    "print(df_clean['is_sarcastic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4246dffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "710047d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and downloading NLTK resources...\n",
      "  Downloading punkt...\n",
      "  ✓ punkt downloaded\n",
      "  Downloading punkt_tab...\n",
      "  ✓ punkt_tab downloaded\n",
      "  Downloading stopwords...\n",
      "  ✓ stopwords downloaded\n",
      "NLTK resources check complete!\n",
      "\n",
      "Loaded 198 stop words\n",
      "Sample stop words: ['only', 'needn', 'all', 'few', 'when', 'because', 'doesn', 'herself', 'such', \"i'd\"]\n",
      "\n",
      "Preprocessing headlines...\n",
      "Sample preprocessed headlines:\n",
      "  Original: former versace store clerk sues over secret 'black code' for minority shoppers\n",
      "  Tokens: ['former', 'versace', 'store', 'clerk', 'sues', 'secret', 'black', 'code', 'minority', 'shoppers']\n",
      "\n",
      "  Original: the 'roseanne' revival catches up to our thorny political mood, for better and worse\n",
      "  Tokens: ['roseanne', 'revival', 'catches', 'thorny', 'political', 'mood', 'better', 'worse']\n",
      "\n",
      "  Original: mom starting to fear son's web series closest thing she will have to grandchild\n",
      "  Tokens: ['mom', 'starting', 'fear', 'son', 'web', 'series', 'closest', 'thing', 'grandchild']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing with stop words removal and tokenization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Download NLTK resources - only if needed\n",
    "print(\"Checking and downloading NLTK resources...\")\n",
    "resources_to_download = ['punkt', 'punkt_tab', 'stopwords']\n",
    "\n",
    "for resource in resources_to_download:\n",
    "    try:\n",
    "        if resource == 'stopwords':\n",
    "            stopwords.words('english')\n",
    "        else:\n",
    "            nltk.data.find(f'tokenizers/{resource}')\n",
    "        print(f\"  ✓ {resource} already available\")\n",
    "    except LookupError:\n",
    "        try:\n",
    "            print(f\"  Downloading {resource}...\")\n",
    "            nltk.download(resource, quiet=True)\n",
    "            print(f\"  ✓ {resource} downloaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ Could not download {resource}: {e}\")\n",
    "\n",
    "print(\"NLTK resources check complete!\")\n",
    "\n",
    "# Get stop words (English)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"\\nLoaded {len(stop_words)} stop words\")\n",
    "print(f\"Sample stop words: {list(stop_words)[:10]}\")\n",
    "\n",
    "# Preprocessing function: lowercase, remove punctuation, tokenize, remove stop words\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and tokenize text, removing stop words\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words and empty tokens\n",
    "    tokens = [t for t in tokens if t.strip() and t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to headlines\n",
    "print(\"\\nPreprocessing headlines...\")\n",
    "df_clean['headline_tokens'] = df_clean['headline'].apply(preprocess_text)\n",
    "print(\"Sample preprocessed headlines:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Original: {df_clean['headline'].iloc[i]}\")\n",
    "    print(f\"  Tokens: {df_clean['headline_tokens'].iloc[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cf13081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer fitted on 26709 headlines\n",
      "Total unique words (vocab size): 25220\n",
      "OOV token index: 1\n",
      "\n",
      "Top 20 most common words:\n",
      "  <OOV>: 1\n",
      "  trump: 2\n",
      "  new: 3\n",
      "  man: 4\n",
      "  year: 5\n",
      "  one: 6\n",
      "  report: 7\n",
      "  area: 8\n",
      "  woman: 9\n",
      "  donald: 10\n",
      "  day: 11\n",
      "  u: 12\n",
      "  says: 13\n",
      "  time: 14\n",
      "  first: 15\n",
      "  obama: 16\n",
      "  like: 17\n",
      "  women: 18\n",
      "  people: 19\n",
      "  get: 20\n",
      "\n",
      "Max headline length: 27\n",
      "Padded sequences shape: (26709, 27)\n",
      "First 3 padded sequences shape: (3, 27)\n",
      "\n",
      "Max headline length: 27\n",
      "Padded sequences shape: (26709, 27)\n",
      "First 3 padded sequences shape: (3, 27)\n"
     ]
    }
   ],
   "source": [
    "# Fit tokenizer on preprocessed headlines\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizer with OOV token\n",
    "tokenizer_sarcasm = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "\n",
    "# Fit on the preprocessed token lists (convert back to strings for Tokenizer)\n",
    "headline_strings = [' '.join(tokens) for tokens in df_clean['headline_tokens']]\n",
    "tokenizer_sarcasm.fit_on_texts(headline_strings)\n",
    "\n",
    "print(f\"Tokenizer fitted on {len(headline_strings)} headlines\")\n",
    "print(f\"Total unique words (vocab size): {len(tokenizer_sarcasm.word_index)}\")\n",
    "print(f\"OOV token index: {tokenizer_sarcasm.word_index.get('<OOV>')}\")\n",
    "print(f\"\\nTop 20 most common words:\")\n",
    "sorted_words = sorted(tokenizer_sarcasm.word_index.items(), key=lambda x: x[1])[:20]\n",
    "for word, idx in sorted_words:\n",
    "    print(f\"  {word}: {idx}\")\n",
    "\n",
    "# Convert headlines to sequences\n",
    "sequences_sarcasm = tokenizer_sarcasm.texts_to_sequences(headline_strings)\n",
    "\n",
    "# Pad sequences (use appropriate maxlen based on data)\n",
    "max_len = max(len(seq) for seq in sequences_sarcasm) if sequences_sarcasm else 100\n",
    "max_len = min(max_len, 100)  # Cap at 100 for practical purposes\n",
    "padded_sarcasm = pad_sequences(sequences_sarcasm, maxlen=max_len, padding='post')\n",
    "\n",
    "print(f\"\\nMax headline length: {max_len}\")\n",
    "print(f\"Padded sequences shape: {padded_sarcasm.shape}\")\n",
    "print(f\"First 3 padded sequences shape: {padded_sarcasm[:3].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e7fd309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape (X): (26709, 27)\n",
      "Target shape (y): (26709,)\n",
      "\n",
      "Target distribution:\n",
      "  Sarcastic (1): 11724 (43.9%)\n",
      "  Non-sarcastic (0): 14985 (56.1%)\n",
      "\n",
      "Model dataset shape: (26709, 28)\n",
      "First few rows:\n",
      "    tok_0   tok_1   tok_2   tok_3   tok_4   tok_5   tok_6   tok_7   tok_8  \\\n",
      "0   220.0     1.0   543.0  3034.0  2201.0   273.0    35.0  1995.0  2498.0   \n",
      "1     1.0  3262.0  2675.0     1.0   309.0  2829.0   164.0   892.0     0.0   \n",
      "2    58.0   749.0   727.0   144.0  1996.0   485.0  4567.0   129.0     1.0   \n",
      "3  1240.0   138.0   260.0  1592.0   224.0  2830.0  1294.0     1.0   790.0   \n",
      "4   669.0   597.0  3784.0   819.0     1.0   463.0   464.0  1163.0    36.0   \n",
      "\n",
      "   tok_9  ...  tok_18  tok_19  tok_20  tok_21  tok_22  tok_23  tok_24  tok_25  \\\n",
      "0    1.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1    0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2    0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3    0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4    0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   tok_26  is_sarcastic  \n",
      "0     0.0             0  \n",
      "1     0.0             0  \n",
      "2     0.0             1  \n",
      "3     0.0             1  \n",
      "4     0.0             0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create features (X) and target (y) dataset\n",
    "X_sarcasm = padded_sarcasm.astype('float32')\n",
    "y_sarcasm = df_clean['is_sarcastic'].values.astype('int32')\n",
    "\n",
    "print(f\"Features shape (X): {X_sarcasm.shape}\")\n",
    "print(f\"Target shape (y): {y_sarcasm.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Sarcastic (1): {(y_sarcasm == 1).sum()} ({100 * (y_sarcasm == 1).sum() / len(y_sarcasm):.1f}%)\")\n",
    "print(f\"  Non-sarcastic (0): {(y_sarcasm == 0).sum()} ({100 * (y_sarcasm == 0).sum() / len(y_sarcasm):.1f}%)\")\n",
    "\n",
    "# Create a DataFrame for convenience\n",
    "feature_cols = [f'tok_{i}' for i in range(X_sarcasm.shape[1])]\n",
    "X_df = pd.DataFrame(X_sarcasm, columns=feature_cols)\n",
    "df_model = X_df.copy()\n",
    "df_model['is_sarcastic'] = y_sarcasm\n",
    "\n",
    "print(f\"\\nModel dataset shape: {df_model.shape}\")\n",
    "print(f\"First few rows:\")\n",
    "print(df_model.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d75f78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 18696 samples\n",
      "Test set: 8013 samples\n",
      "\n",
      "Training logistic regression...\n",
      "Training complete!\n",
      "\n",
      "============================================================\n",
      "MODEL EVALUATION\n",
      "============================================================\n",
      "Accuracy: 0.5657\n",
      "ROC AUC: 0.5623\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3074 1422]\n",
      " [2058 1459]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Sarcastic       0.60      0.68      0.64      4496\n",
      "    Sarcastic       0.51      0.41      0.46      3517\n",
      "\n",
      "     accuracy                           0.57      8013\n",
      "    macro avg       0.55      0.55      0.55      8013\n",
      " weighted avg       0.56      0.57      0.56      8013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sarcasm, y_sarcasm, test_size=0.3, stratify=y_sarcasm, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Train logistic regression with increased iterations\n",
    "clf_sarcasm = LogisticRegression(max_iter=5000, random_state=42, class_weight='balanced', solver='lbfgs')\n",
    "print(\"\\nTraining logistic regression...\")\n",
    "clf_sarcasm.fit(X_train, y_train)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf_sarcasm.predict(X_test)\n",
    "y_pred_proba = clf_sarcasm.predict_proba(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba[:, 1]):.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Non-Sarcastic', 'Sarcastic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "947094ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EMBEDDING LAYER APPROACH\n",
      "======================================================================\n",
      "\n",
      "Embedding Configuration:\n",
      "  Vocabulary size: 5001\n",
      "  Embedding dimension: 64\n",
      "  Input sequence length: 27\n",
      "\n",
      "Converting sequences to embeddings...\n",
      "Embedded data shape: (26709, 27, 64)\n",
      "Flattened embeddings shape (average pooling): (26709, 64)\n",
      "\n",
      "Train set (embedded): 18696 samples\n",
      "Test set (embedded): 8013 samples\n"
     ]
    }
   ],
   "source": [
    "# Create embedding layer and convert sequences to embeddings\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EMBEDDING LAYER APPROACH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create embedding model\n",
    "embedding_dim = 64  # Dimension of embedding vectors\n",
    "vocab_size = 5000 + 1  # +1 for padding index\n",
    "\n",
    "embedding_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)\n",
    "])\n",
    "\n",
    "print(f\"\\nEmbedding Configuration:\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "print(f\"  Input sequence length: {max_len}\")\n",
    "\n",
    "# Convert padded sequences to embeddings\n",
    "print(\"\\nConverting sequences to embeddings...\")\n",
    "X_embedded = embedding_model.predict(padded_sarcasm, verbose=0)\n",
    "print(f\"Embedded data shape: {X_embedded.shape}\")\n",
    "\n",
    "# Flatten embeddings for logistic regression (average pooling)\n",
    "X_embedded_flat = X_embedded.mean(axis=1)  # Average across sequence length\n",
    "print(f\"Flattened embeddings shape (average pooling): {X_embedded_flat.shape}\")\n",
    "\n",
    "# Split dataset using embedded features\n",
    "X_train_emb, X_test_emb, y_train_emb, y_test_emb = train_test_split(\n",
    "    X_embedded_flat, y_sarcasm, test_size=0.3, stratify=y_sarcasm, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set (embedded): {X_train_emb.shape[0]} samples\")\n",
    "print(f\"Test set (embedded): {X_test_emb.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5987f7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOGISTIC REGRESSION ON EMBEDDED DATA\n",
      "======================================================================\n",
      "\n",
      "Training logistic regression on embeddings...\n",
      "Training complete!\n",
      "\n",
      "============================================================\n",
      "EMBEDDED MODEL EVALUATION\n",
      "============================================================\n",
      "Accuracy: 0.5540\n",
      "ROC AUC: 0.5930\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2526 1970]\n",
      " [1604 1913]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Sarcastic       0.61      0.56      0.59      4496\n",
      "    Sarcastic       0.49      0.54      0.52      3517\n",
      "\n",
      "     accuracy                           0.55      8013\n",
      "    macro avg       0.55      0.55      0.55      8013\n",
      " weighted avg       0.56      0.55      0.56      8013\n",
      "\n",
      "\n",
      "============================================================\n",
      "COMPARISON: ORIGINAL vs EMBEDDED\n",
      "============================================================\n",
      "Original Accuracy:  0.5657\n",
      "Embedded Accuracy:  0.5540\n",
      "Improvement:        -1.17%\n",
      "\n",
      "Original ROC AUC:   0.5623\n",
      "Embedded ROC AUC:   0.5930\n",
      "Improvement:        +0.0307\n"
     ]
    }
   ],
   "source": [
    "# Retrain logistic regression on embedded data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOGISTIC REGRESSION ON EMBEDDED DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train logistic regression on embeddings\n",
    "clf_embedded = LogisticRegression(max_iter=5000, random_state=42, class_weight='balanced', solver='lbfgs')\n",
    "print(\"\\nTraining logistic regression on embeddings...\")\n",
    "clf_embedded.fit(X_train_emb, y_train_emb)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Evaluate on embedded features\n",
    "y_pred_emb = clf_embedded.predict(X_test_emb)\n",
    "y_pred_proba_emb = clf_embedded.predict_proba(X_test_emb)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMBEDDED MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "acc_embedded = accuracy_score(y_test_emb, y_pred_emb)\n",
    "roc_embedded = roc_auc_score(y_test_emb, y_pred_proba_emb[:, 1])\n",
    "\n",
    "print(f\"Accuracy: {acc_embedded:.4f}\")\n",
    "print(f\"ROC AUC: {roc_embedded:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_test_emb, y_pred_emb)}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_emb, y_pred_emb, target_names=['Non-Sarcastic', 'Sarcastic']))\n",
    "\n",
    "# Compare with original model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: ORIGINAL vs EMBEDDED\")\n",
    "print(\"=\"*60)\n",
    "acc_original = accuracy_score(y_test, y_pred)\n",
    "roc_original = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "print(f\"Original Accuracy:  {acc_original:.4f}\")\n",
    "print(f\"Embedded Accuracy:  {acc_embedded:.4f}\")\n",
    "print(f\"Improvement:        {(acc_embedded - acc_original)*100:+.2f}%\")\n",
    "print(f\"\\nOriginal ROC AUC:   {roc_original:.4f}\")\n",
    "print(f\"Embedded ROC AUC:   {roc_embedded:.4f}\")\n",
    "print(f\"Improvement:        {(roc_embedded - roc_original):+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
