{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9e0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tokenizer and TextVectorization from TensorFlow/Keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d39bc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index: {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "\n",
      "Sequences:\n",
      "i love my dog -> [4, 2, 1, 3]\n",
      "i love my cat -> [4, 2, 1, 6]\n",
      "you love my dog! -> [5, 2, 1, 3]\n",
      "Do you think my dog is amazing? -> [7, 5, 8, 1, 3, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Fit the Tokenizer on phrases\n",
    "phrases = [\"i love my dog\", \"i love my cat\", \"you love my dog!\" , \"Do you think my dog is amazing?\"]\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(phrases)\n",
    "\n",
    "# Display the word index\n",
    "print(\"Word Index:\", tokenizer.word_index)\n",
    "print(\"\\nSequences:\")\n",
    "sequences = tokenizer.texts_to_sequences(phrases)\n",
    "for phrase, seq in zip(phrases, sequences):\n",
    "    print(f\"{phrase} -> {seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b1e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index: {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "\n",
      "Padded Sequences:\n",
      "[[ 0  4  2  1  3]\n",
      " [ 0  4  2  1  6]\n",
      " [ 0  5  2  1  3]\n",
      " [ 8  1  3  9 10]]\n"
     ]
    }
   ],
   "source": [
    "padded=pad_sequences(sequences, maxlen=5)\n",
    "print(\"Word Index:\", tokenizer.word_index)\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4888bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New Sequences:\n",
      "[[1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "new_phrases = [\"my dog loves my computer\",]\n",
    "new_sequences = tokenizer.texts_to_sequences(new_phrases)\n",
    "print(\"\\nNew Sequences:\")\n",
    "print(new_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c89f4daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   phrase label\n",
      "0      J'adore ce produit   bon\n",
      "1        Très bon service   bon\n",
      "2  Je suis très satisfait   bon\n",
      "3      Excellente qualité   bon\n",
      "4  Je recommande vivement   bon\n"
     ]
    }
   ],
   "source": [
    "good_phrases = [\n",
    "    \"J'adore ce produit\",\n",
    "    \"Très bon service\",\n",
    "    \"Je suis très satisfait\",\n",
    "    \"Excellente qualité\",\n",
    "    \"Je recommande vivement\",\n",
    "    \"C'est parfait\",\n",
    "    \"Super expérience\",\n",
    "    \"Service client génial\",\n",
    "    \"Produit conforme à la description\",\n",
    "    \"Très heureux de mon achat\",\n",
    "]\n",
    "\n",
    "bad_phrases = [\n",
    "    \"Je suis très déçu\",\n",
    "    \"Mauvaise qualité\",\n",
    "    \"Ça ne marche pas\",\n",
    "    \"Service client lamentable\",\n",
    "    \"Ne recommande pas\",\n",
    "    \"Très mauvaise expérience\",\n",
    "    \"Produit cassé\",\n",
    "    \"Perte de temps\",\n",
    "    \"C'était nul\",\n",
    "    \"Arnaque totale\",\n",
    "]\n",
    "\n",
    "db_sentences = good_phrases + bad_phrases\n",
    "db_labels = ['bon'] * len(good_phrases) + ['mauvais'] * len(bad_phrases)\n",
    "#create df from sentences and labels\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'phrase': db_sentences, 'label': db_labels})\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4416250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer fitted on database phrases\n",
      "Total unique words in database: 42\n",
      "OOV token index: 1\n",
      "\n",
      "Word Index (first 20 words):\n",
      "  <OOV>: 1\n",
      "  très: 2\n",
      "  produit: 3\n",
      "  service: 4\n",
      "  je: 5\n",
      "  suis: 6\n",
      "  qualité: 7\n",
      "  recommande: 8\n",
      "  expérience: 9\n",
      "  client: 10\n",
      "  de: 11\n",
      "  mauvaise: 12\n",
      "  ne: 13\n",
      "  pas: 14\n",
      "  j'adore: 15\n",
      "  ce: 16\n",
      "  bon: 17\n",
      "  satisfait: 18\n",
      "  excellente: 19\n",
      "  vivement: 20\n",
      "\n",
      "Database sequences shape: (20, 15)\n",
      "First 3 padded sequences:\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0 15 16  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  2 17  4]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  5  6  2 18]]\n",
      "   tok_0  tok_1  tok_2  tok_3  tok_4  tok_5  tok_6  tok_7  tok_8  tok_9  \\\n",
      "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "   tok_10  tok_11  tok_12  tok_13  tok_14  label  \n",
      "0     0.0     0.0    15.0    16.0     3.0      1  \n",
      "1     0.0     0.0     2.0    17.0     4.0      1  \n",
      "2     0.0     5.0     6.0     2.0    18.0      1  \n",
      "3     0.0     0.0     0.0    19.0     7.0      1  \n",
      "4     0.0     0.0     5.0     8.0    20.0      1  \n"
     ]
    }
   ],
   "source": [
    "# Fit tokenizer on the database phrases\n",
    "# Add oov token to handle out-of-vocabulary words\n",
    "tokenizer_db = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer_db.fit_on_texts(db_sentences)\n",
    "\n",
    "print(\"Tokenizer fitted on database phrases\")\n",
    "print(f\"Total unique words in database: {len(tokenizer_db.word_index)}\")\n",
    "print(f\"OOV token index: {tokenizer_db.word_index.get('<OOV>')}\")\n",
    "print(f\"\\nWord Index (first 20 words):\")\n",
    "sorted_words = sorted(tokenizer_db.word_index.items(), key=lambda x: x[1])[:20]\n",
    "for word, idx in sorted_words:\n",
    "    print(f\"  {word}: {idx}\")\n",
    "\n",
    "# Convert all phrases to sequences\n",
    "db_sequences = tokenizer_db.texts_to_sequences(db_sentences)\n",
    "db_padded = pad_sequences(db_sequences, maxlen=15)\n",
    "\n",
    "print(f\"\\nDatabase sequences shape: {db_padded.shape}\")\n",
    "print(f\"First 3 padded sequences:\\n{db_padded[:3]}\")\n",
    "\n",
    "# create a df for logistic regression model: vecteur et label\n",
    "# features will be the padded sequences (flattened) and target is numeric label\n",
    "import numpy as np\n",
    "X = db_padded.astype('float32')\n",
    "y = np.array([1 if label == 'bon' else 0 for label in db_labels])\n",
    "\n",
    "# Create a DataFrame with features for exploration if you like\n",
    "feature_cols = [f'tok_{i}' for i in range(X.shape[1])]\n",
    "import pandas as pd\n",
    "X_df = pd.DataFrame(X, columns=feature_cols)\n",
    "df_lr = X_df.copy()\n",
    "df_lr['label'] = y\n",
    "print(df_lr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8d0b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify new phrases using the fitted tokenizer\n",
    "new_test_phrases = [\n",
    "    \"J'adore ce service, excellent!\",\n",
    "    \"C'est horrible, très mauvais\",\n",
    "    \"Super produit, je suis satisfait\",\n",
    "    \"Arnaque, c'est cassé\",\n",
    "    \"Très bon, je recommande\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bff7a67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (16, 15), Test shape: (4, 15)\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mauvais       0.67      1.00      0.80         2\n",
      "         bon       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.83      0.75      0.73         4\n",
      "weighted avg       0.83      0.75      0.73         4\n",
      "\n",
      "Accuracy: 0.75\n",
      "Confusion matrix:\n",
      " [[2 0]\n",
      " [1 1]]\n",
      "\n",
      "Train accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate logistic regression on padded token sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# Fit logistic regression\n",
    "clf = LogisticRegression(max_iter=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['mauvais','bon']))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save model for reuse (optional)\n",
    "import joblib\n",
    "joblib.dump(clf, 'logreg_token_model.joblib')\n",
    "\n",
    "# Use the model for predicting the training set for quick sanity check\n",
    "train_pred = clf.predict(X_train)\n",
    "print('\\nTrain accuracy:', accuracy_score(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba2b40e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for new phrases:\n",
      "\n",
      "Phrase: J'adore ce service, excellent!\n",
      "Sequence: [15, 16, 4, 1]\n",
      "Padded: [ 0  0  0  0  0  0  0  0  0  0  0 15 16  4  1]\n",
      "Predicted label: bon  (prob: 0.619)\n",
      "---\n",
      "\n",
      "Phrase: C'est horrible, très mauvais\n",
      "Sequence: [21, 1, 2, 1]\n",
      "Padded: [ 0  0  0  0  0  0  0  0  0  0  0 21  1  2  1]\n",
      "Predicted label: bon  (prob: 0.699)\n",
      "---\n",
      "\n",
      "Phrase: Super produit, je suis satisfait\n",
      "Sequence: [23, 3, 5, 6, 18]\n",
      "Padded: [ 0  0  0  0  0  0  0  0  0  0 23  3  5  6 18]\n",
      "Predicted label: bon  (prob: 1.0)\n",
      "---\n",
      "\n",
      "Phrase: Arnaque, c'est cassé\n",
      "Sequence: [41, 21, 36]\n",
      "Padded: [ 0  0  0  0  0  0  0  0  0  0  0  0 41 21 36]\n",
      "Predicted label: mauvais  (prob: 0.893)\n",
      "---\n",
      "\n",
      "Phrase: Très bon, je recommande\n",
      "Sequence: [2, 17, 5, 8]\n",
      "Padded: [ 0  0  0  0  0  0  0  0  0  0  0  2 17  5  8]\n",
      "Predicted label: bon  (prob: 0.562)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "new_sequences = tokenizer_db.texts_to_sequences(new_test_phrases)\n",
    "new_padded = pad_sequences(new_sequences, maxlen=15)\n",
    "\n",
    "preds = clf.predict(new_padded)\n",
    "probs = clf.predict_proba(new_padded)\n",
    "label_map = {1: 'bon', 0: 'mauvais'}\n",
    "\n",
    "print('\\nPredictions for new phrases:')\n",
    "for phrase, seq, padded_seq, pred, prob in zip(new_test_phrases, new_sequences, new_padded, preds, probs):\n",
    "    print('\\nPhrase:', phrase)\n",
    "    print('Sequence:', seq)\n",
    "    print('Padded:', padded_seq)\n",
    "    print('Predicted label:', label_map[pred], f' (prob: {np.round(prob.max(), 3)})')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8dce49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
